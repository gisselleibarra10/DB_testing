// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct StartMedicalStreamTranscriptionInput: Swift.Equatable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: TranscribeStreamingClientTypes.AudioStream?
    /// Labels all personal health information (PHI) identified in your transcript. Content identification is performed at the segment level; PHI is flagged upon complete transcription of an audio segment. For more information, see [Identifying personal health information (PHI) in a transcription](https://docs.aws.amazon.com/transcribe/latest/dg/phi-id.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool
    /// Specify the language code that represents the language spoken in your audio. Amazon Transcribe Medical only supports US English (en-US).
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the encoding used for the input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Amazon Transcribe Medical supports a range from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. Up to two channels are supported.
    public var numberOfChannels: Swift.Int?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe Medical generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool
    /// Specify the medical specialty contained in your audio.
    /// This member is required.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Specify the type of input audio. For example, choose DICTATION for a provider dictating patient notes and CONVERSATION for a dialogue between a patient and a medical professional.
    /// This member is required.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive.
    public var vocabularyName: Swift.String?

    public init (
        audioStream: TranscribeStreamingClientTypes.AudioStream? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.type = type
        self.vocabularyName = vocabularyName
    }
}
