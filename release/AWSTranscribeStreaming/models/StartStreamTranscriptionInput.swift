// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct StartStreamTranscriptionInput: Swift.Equatable {
    /// PCM-encoded stream of audio blobs. The audio stream is encoded as an HTTP/2 data frame.
    /// This member is required.
    public var audioStream: TranscribeStreamingClientTypes.AudioStream?
    /// Set this field to PII to identify personally identifiable information (PII) in the transcription output. Content identification is performed only upon complete transcription of the audio segments. You can’t set both ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Set this field to PII to redact personally identifiable information (PII) in the transcription output. Content redaction is performed only upon complete transcription of the audio segments. You can’t set both ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// When true, instructs Amazon Transcribe to process each audio channel separately, then merges the transcription output of each channel into a single transcription. Amazon Transcribe also produces a transcription of each item. An item includes the start time, end time, and any alternative transcriptions.
    public var enableChannelIdentification: Swift.Bool
    /// When true, instructs Amazon Transcribe to present transcription results that have the partial results stabilized. Normally, any word or phrase from one partial result can change in a subsequent partial result. With partial results stabilization enabled, only the last few words of one partial result can change in another partial result.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Optional. Set this value to true to enable language identification for your media stream.
    public var identifyLanguage: Swift.Bool
    /// The language code of the input audio stream.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// The name of the language model you want to use.
    public var languageModelName: Swift.String?
    /// An object containing a list of languages that might be present in your audio. You must provide two or more language codes to help Amazon Transcribe identify the correct language of your media stream with the highest possible accuracy. You can only select one variant per language; for example, you can't include both en-US and en-UK in the same request. You can only use this parameter if you've set IdentifyLanguage to truein your request.
    public var languageOptions: Swift.String?
    /// The encoding used for the input audio.
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in Hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// The number of channels that are in your audio stream.
    public var numberOfChannels: Swift.Int?
    /// You can use this field to set the stability level of the transcription results. A higher stability level means that the transcription results are less likely to change. Higher stability levels can come with lower overall transcription accuracy.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// List the PII entity types you want to identify or redact. In order to specify entity types, you must have either ContentIdentificationType or ContentRedactionType enabled. PIIEntityTypes must be comma-separated; the available values are: BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_NUMBER, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, PIN, EMAIL, ADDRESS, NAME, PHONE, SSN, and ALL. PiiEntityTypes is an optional parameter with a default value of ALL.
    public var piiEntityTypes: Swift.String?
    /// Optional. From the subset of languages codes you provided for LanguageOptions, you can select one preferred language for your transcription. You can only use this parameter if you've set IdentifyLanguage to truein your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// A identifier for the transcription session. Use this parameter when you want to retry a session. If you don't provide a session ID, Amazon Transcribe will generate one for you and return it in the response.
    public var sessionId: Swift.String?
    /// When true, enables speaker identification in your media stream.
    public var showSpeakerLabel: Swift.Bool
    /// The manner in which you use your vocabulary filter to filter words in your transcript. Remove removes filtered words from your transcription results. Mask masks filtered words with a *** in your transcription results. Tag keeps the filtered words in your transcription results and tags them. The tag appears as VocabularyFilterMatch equal to True.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// The name of the vocabulary filter you want to use with your transcription. This operation is not intended for use in conjunction with the IdentifyLanguage operation. If you're using IdentifyLanguage in your request and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames operation instead.
    public var vocabularyFilterName: Swift.String?
    /// The names of the vocabulary filters you want to use with your transcription. Note that if the vocabulary filters you specify are in languages that don't match the language identified in your media, your job fails. This operation is only intended for use in conjunction with the IdentifyLanguage operation. If you're not using IdentifyLanguage in your request and want to use a vocabulary filter with your transcription, use the VocabularyFilterName operation instead.
    public var vocabularyFilterNames: Swift.String?
    /// The name of the custom vocabulary you want to use with your transcription. This operation is not intended for use in conjunction with the IdentifyLanguage operation. If you're using IdentifyLanguage in your request and want to use one or more custom vocabularies with your transcription, use the VocabularyNames operation instead.
    public var vocabularyName: Swift.String?
    /// The names of the custom vocabularies you want to use with your transcription. Note that if the custom vocabularies you specify are in languages that don't match the language identified in your media, your job fails. This operation is only intended for use in conjunction with the IdentifyLanguage operation. If you're not using IdentifyLanguage in your request and want to use a custom vocabulary with your transcription, use the VocabularyName operation instead.
    public var vocabularyNames: Swift.String?

    public init (
        audioStream: TranscribeStreamingClientTypes.AudioStream? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        enablePartialResultsStabilization: Swift.Bool = false,
        identifyLanguage: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}
