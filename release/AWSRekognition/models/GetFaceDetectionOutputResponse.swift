// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct GetFaceDetectionOutputResponse: Swift.Equatable {
    /// An array of faces detected in the video. Each element contains a detected face's details and the time, in milliseconds from the start of the video, the face was detected.
    public var faces: [RekognitionClientTypes.FaceDetection]?
    /// The current status of the face detection job.
    public var jobStatus: RekognitionClientTypes.VideoJobStatus?
    /// If the response is truncated, Amazon Rekognition returns this token that you can use in the subsequent request to retrieve the next set of faces.
    public var nextToken: Swift.String?
    /// If the job fails, StatusMessage provides a descriptive error message.
    public var statusMessage: Swift.String?
    /// Information about a video that Amazon Rekognition Video analyzed. Videometadata is returned in every page of paginated responses from a Amazon Rekognition video operation.
    public var videoMetadata: RekognitionClientTypes.VideoMetadata?

    public init (
        faces: [RekognitionClientTypes.FaceDetection]? = nil,
        jobStatus: RekognitionClientTypes.VideoJobStatus? = nil,
        nextToken: Swift.String? = nil,
        statusMessage: Swift.String? = nil,
        videoMetadata: RekognitionClientTypes.VideoMetadata? = nil
    )
    {
        self.faces = faces
        self.jobStatus = jobStatus
        self.nextToken = nextToken
        self.statusMessage = statusMessage
        self.videoMetadata = videoMetadata
    }
}
