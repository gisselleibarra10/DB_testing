// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct GetFaceSearchOutputResponse: Swift.Equatable {
    /// The current status of the face search job.
    public var jobStatus: RekognitionClientTypes.VideoJobStatus?
    /// If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results.
    public var nextToken: Swift.String?
    /// An array of persons, [PersonMatch], in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to StartFaceSearch. Each Persons element includes a time the person was matched, face match details (FaceMatches) for matching faces in the collection, and person information (Person) for the matched person.
    public var persons: [RekognitionClientTypes.PersonMatch]?
    /// If the job fails, StatusMessage provides a descriptive error message.
    public var statusMessage: Swift.String?
    /// Information about a video that Amazon Rekognition analyzed. Videometadata is returned in every page of paginated responses from a Amazon Rekognition Video operation.
    public var videoMetadata: RekognitionClientTypes.VideoMetadata?

    public init (
        jobStatus: RekognitionClientTypes.VideoJobStatus? = nil,
        nextToken: Swift.String? = nil,
        persons: [RekognitionClientTypes.PersonMatch]? = nil,
        statusMessage: Swift.String? = nil,
        videoMetadata: RekognitionClientTypes.VideoMetadata? = nil
    )
    {
        self.jobStatus = jobStatus
        self.nextToken = nextToken
        self.persons = persons
        self.statusMessage = statusMessage
        self.videoMetadata = videoMetadata
    }
}
