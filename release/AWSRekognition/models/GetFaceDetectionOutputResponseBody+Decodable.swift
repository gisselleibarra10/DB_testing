// Code generated by smithy-swift-codegen. DO NOT EDIT!

import ClientRuntime

struct GetFaceDetectionOutputResponseBody: Swift.Equatable {
    let jobStatus: RekognitionClientTypes.VideoJobStatus?
    let statusMessage: Swift.String?
    let videoMetadata: RekognitionClientTypes.VideoMetadata?
    let nextToken: Swift.String?
    let faces: [RekognitionClientTypes.FaceDetection]?
}

extension GetFaceDetectionOutputResponseBody: Swift.Decodable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case faces = "Faces"
        case jobStatus = "JobStatus"
        case nextToken = "NextToken"
        case statusMessage = "StatusMessage"
        case videoMetadata = "VideoMetadata"
    }

    public init (from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let jobStatusDecoded = try containerValues.decodeIfPresent(RekognitionClientTypes.VideoJobStatus.self, forKey: .jobStatus)
        jobStatus = jobStatusDecoded
        let statusMessageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .statusMessage)
        statusMessage = statusMessageDecoded
        let videoMetadataDecoded = try containerValues.decodeIfPresent(RekognitionClientTypes.VideoMetadata.self, forKey: .videoMetadata)
        videoMetadata = videoMetadataDecoded
        let nextTokenDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .nextToken)
        nextToken = nextTokenDecoded
        let facesContainer = try containerValues.decodeIfPresent([RekognitionClientTypes.FaceDetection?].self, forKey: .faces)
        var facesDecoded0:[RekognitionClientTypes.FaceDetection]? = nil
        if let facesContainer = facesContainer {
            facesDecoded0 = [RekognitionClientTypes.FaceDetection]()
            for structure0 in facesContainer {
                if let structure0 = structure0 {
                    facesDecoded0?.append(structure0)
                }
            }
        }
        faces = facesDecoded0
    }
}
