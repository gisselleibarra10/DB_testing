// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct GetSegmentDetectionOutputResponse: Swift.Equatable {
    /// An array of objects. There can be multiple audio streams. Each AudioMetadata object contains metadata for a single audio stream. Audio information in an AudioMetadata objects includes the audio codec, the number of audio channels, the duration of the audio stream, and the sample rate. Audio metadata is returned in each page of information returned by GetSegmentDetection.
    public var audioMetadata: [RekognitionClientTypes.AudioMetadata]?
    /// Current status of the segment detection job.
    public var jobStatus: RekognitionClientTypes.VideoJobStatus?
    /// If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination token in the response. You can use this pagination token to retrieve the next set of text.
    public var nextToken: Swift.String?
    /// An array of segments detected in a video. The array is sorted by the segment types (TECHNICAL_CUE or SHOT) specified in the SegmentTypes input parameter of StartSegmentDetection. Within each segment type the array is sorted by timestamp values.
    public var segments: [RekognitionClientTypes.SegmentDetection]?
    /// An array containing the segment types requested in the call to StartSegmentDetection.
    public var selectedSegmentTypes: [RekognitionClientTypes.SegmentTypeInfo]?
    /// If the job fails, StatusMessage provides a descriptive error message.
    public var statusMessage: Swift.String?
    /// Currently, Amazon Rekognition Video returns a single object in the VideoMetadata array. The object contains information about the video stream in the input file that Amazon Rekognition Video chose to analyze. The VideoMetadata object includes the video codec, video format and other information. Video metadata is returned in each page of information returned by GetSegmentDetection.
    public var videoMetadata: [RekognitionClientTypes.VideoMetadata]?

    public init (
        audioMetadata: [RekognitionClientTypes.AudioMetadata]? = nil,
        jobStatus: RekognitionClientTypes.VideoJobStatus? = nil,
        nextToken: Swift.String? = nil,
        segments: [RekognitionClientTypes.SegmentDetection]? = nil,
        selectedSegmentTypes: [RekognitionClientTypes.SegmentTypeInfo]? = nil,
        statusMessage: Swift.String? = nil,
        videoMetadata: [RekognitionClientTypes.VideoMetadata]? = nil
    )
    {
        self.audioMetadata = audioMetadata
        self.jobStatus = jobStatus
        self.nextToken = nextToken
        self.segments = segments
        self.selectedSegmentTypes = selectedSegmentTypes
        self.statusMessage = statusMessage
        self.videoMetadata = videoMetadata
    }
}
